{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Serving (30 points)\n",
    "\n",
    "\n",
    "In this task, we will explore different ways to serve your model. Most of your assignments in other DS/ML courses are script based. So you probably trained some model, did some predictions and produced some output. However, in production, you need a more systematic way to run your classifiers. We will discuss four different ways for running your classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a : Command Line (7.5 points)\n",
    "\n",
    "The simplest possible mechanism is to serve your model as a command line script. So, you will call your script with some parameters (such as classifier name, features etc) and the model will output the prediction in the terminal.\n",
    "\n",
    "This might sound like a retro and clumsy way of serving things. However, it has lot of practical usecases. For example, you can use the full power of Unix command line tools to slice and dice the model predictions which might require complex coding otherwise. You can also use this to test that your model is working as expected before deploying it to production. A number of companies (including mine) have CI/CD pipelines that test your model in your terminal using sophisticated shell scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a command line utility with Typer Library\n",
    "\n",
    "In this assignment, we will implement a very simple scenario. However, things will be much more complex in production. So, it is useful to develop this utility in a good library.\n",
    "\n",
    "The classical way is to use the [argparse](https://docs.python.org/3/library/argparse.html) library that comes with standard Python. This is often sufficient if your requirements are simple enough. However, there are a number of advanced features that are missing or hard to use in argparse. \n",
    "\n",
    "A more modern way is to use the [click](https://click.palletsprojects.com). This is a an advanced and very powerful tool that allows you to create sophisticated command line utilities. I have some click scripts that automate the entire ML pipeline. While it is quite powerful, it also has a steep learning curve. Nevertheless, I would definitely recommend you to learn this library.\n",
    "\n",
    "In this assignment, we will take an intermediate approach of using the [typer](https://typer.tiangolo.com/) library. Typer is based on click library and does some cool syntactic sugars to make life much easier. Usually, you can get 90% of what Click offers by using typer. \n",
    "\n",
    "For this assignment, you have to skim four concepts from Typer. \n",
    "\n",
    "1. Commands : More details [here](https://typer.tiangolo.com/tutorial/commands/). \n",
    "\n",
    "2. Arguments : More details [here](https://typer.tiangolo.com/tutorial/arguments/). \n",
    "\n",
    "3. Options: More details [here](https://typer.tiangolo.com/tutorial/options/). \n",
    "\n",
    "4. Variadic Arguments: More details [here](https://typer.tiangolo.com/tutorial/multiple-values/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Utility Task Specifications\n",
    "\n",
    "1. The code for this task is inside `task2/typer_demo.py` \n",
    "\n",
    "2. You will develop two Typer commands: train and predict. The \"business\" logic for these is already written by me. All you need to do is to convert them to typer commands. \n",
    "\n",
    "3. The train command should have the name `train` (see how to add name to a Typer command) and should have the help string as \"Train a classifier.\". The predict command should have the name `predict` and should have the help string as \"Make a prediction using a trained classifier.\". \n",
    "\n",
    "4. The `predict` command should accept one required argument named `item` and a optional argument named `classifier`. The item should accept a variadic input of floats. In other words, I should be able to pass the feature values as a b c d e etc where a/b/c/d/e are float. The classifier should take values from ValidClassifiers enum that can be found in ds5612_pa2.utils.ml_utils. \n",
    "\n",
    "5. For both `train` and `predict`, annotate the `classifier` option so that it outputs the help message as \"Classifier name.\" You do not need to do anything for item argument.\n",
    "\n",
    "6. You can test your code with the grader using the command\n",
    "\n",
    "> rye test -- -m t2typer\n",
    "\n",
    "7.  You can manually test the code by running the following commands from the project root folder (DS5612_PA2). \n",
    "\n",
    "> rye run typer_task --help \n",
    "> \n",
    "> rye run typer_task train --help\n",
    "> \n",
    "> rye run typer_task predict --help\n",
    ">\n",
    "> rye run typer_task train\n",
    "> \n",
    "> rye run typer_task train --classifier KNN\n",
    ">\n",
    "> rye run typer_task predict --classifier NaiveBayes 1 2 3 4 5 6 7 8 9 0\n",
    "\n",
    "8. Optional: Nerd stuff: Check out the pyproject.toml file's rye script section to see how this custom command is done. You can use the script section of rye/uv/pdm to create scripts that can simplify your life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b: Terminal Application with Textual (7.5 points)\n",
    "\n",
    "The second type of model serving is terminal based. Typically, your ML models will be deployed on a Linux server that does not have any UI. So, you need to be able to do some simple evaluation of your model using your terminal.\n",
    "\n",
    "[Textual](https://textual.textualize.io/) is a popular approach to create very complex terminal applications. In this task, we will develop a terminal variant of the model serving that you did for typer. \n",
    "\n",
    "A screenshot of the application be found below. I have partially implemented the business and styling logic of it. You just need to create the UI part. The application should have the following features. Note that while some ideas are open to experimentation, some (like the element id are fixed as they are needed for automatic grading).\n",
    "\n",
    "The code for this task is inside `task2/textual_demo.py`. You can test it using the command\n",
    "\n",
    "> rye test -- -m t2textual\n",
    "\n",
    "1. It should have two panels: input and output. This can be using a combination of `Horizontal` and `Vertical` respectively.\n",
    "2. It should have a dropdown (`Select`) with the id `classifier` and takes the values from the variable `CLASSIFIER_OPTIONS`\n",
    "3. It should have a `TextArea` with the id `input-text`\n",
    "4. It should have a `Button` with text \"Predict\" and id as `predict`\n",
    "5. The output of the classifier will be done Label and ProgressBar. \n",
    "6. There should be two labels. First should have text \"Positive Score: \" and id `positive_score`. Second should have \"Negative Score: \" and id `negative_score`\n",
    "7. The classifier score should be displayed using ProgressBar. The first should have the id as `positive` and second should have an id as `negative`. They should be able to display a numbers between 0.0 and 1.0. So if I give a value of 0.6, then the progress bar should be 60% highlighted. \n",
    "8. In the on_mount event, reset the value of both progress bars to 0.\n",
    "9. When the predict button is pressed,ensure that the Classifier and features are specified. You can enter a sample feature value as \"1 2 3 4 5 6 7 8 9 0\" (without the quotes). In general, the textarea should accept 10 features that are space separated. If the inputs are not specified, then an error message should be shown using the `notify` command. \n",
    "\n",
    "\n",
    "**Hints**\n",
    "1. You can terminate the application using Ctrl+c.\n",
    "2. You can run the application using a rye command `rye run textual_task`\n",
    "\n",
    "**References**:\n",
    "1. Basic tutorial: [here](https://textual.textualize.io/tutorial/)\n",
    "2. You need to use the following UI types: Horizontal, Vertical, Button, Footer, Header, Label, ProgressBar, Select, TextArea\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Textual Application](resources/textual_screenshot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c: API based Model Serving using FastAPI (7.5 points)\n",
    "\n",
    "The third type of model serving is API based. In this case, you will serve your model as a web service. It should have some predict function that accepts some information required for inference and should return the prediction. \n",
    "\n",
    "Such a model serving is often required when your application consists of multiple microservices one of which is your model. This also allows you to do lot of testing such as by using tools such as [Postman](https://www.postman.com/). \n",
    "\n",
    "We will be using [FastAPI](https://fastapi.tiangolo.com/). It is an exceptional library that is based on Pydantic and [Starlette](https://www.starlette.io/) and is blazing fast. Unless you have a good reason, you should implement your microservices using FastAPI (and not some old methods such as Flask, Django etc).\n",
    "\n",
    "A really cool feature of FastAPI is that it is built on top of Pydantic. So it will automatically validate the HTTP request and automatically convert the input into valid Python objects that you can immediately use in your code. Pydantic also allows you to write complex validations (as you did in Task 1). Not long ago, you will be spending lot of code doing this manually and now FastAPI does this automatically. \n",
    "\n",
    "We will only explore FastAPI briefly. But it is worth learning it in detail. \n",
    "\n",
    "The code for this task is inside `task2/fastapi_demo.py` . You can test it using the command\n",
    "\n",
    "> rye test -- -m t2fastAPI\n",
    "\n",
    "You can run the FastAPI server using the following command. But remember to stop the server before running the test suite.\n",
    "\n",
    "> rye run fastapi_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c1: Modeling Request and Response\n",
    "\n",
    "Create three Pydantic BaseModel classes with the names `PredictRequest`, `PredictionResponse` and `DetailedPredictionResponse`. The request will be an instance of `PredictRequest`. To make things interesting, we will do a minor demonstration of API versioning. We will implement two versions of the API. The V1 will return `PredictionResponse` while the V2 will return `DetailedPredictionResponse`.\n",
    "\n",
    "The `PredictRequest` should have two fields: `classifier` that can take only values from `pipeline_configs.ValidClassifierNames` and `features` that is a list of float.\n",
    "\n",
    "\n",
    "The `PredictionResponse` should have two fields: `predicted_class` that is an integer and `ml_model_version` that is a string with default value of \"V1\". This should allow the clients to understand that they are processing the output of V1 model.\n",
    "\n",
    "The `PredictionResponse` should extend `PredictionResponse` and have a new field: `probabilities` that is a tuple with two floats (ie the probability for positive and negative class). Additionally, it should set the  `ml_model_version` as a string with default value of \"V2\". This should allow the clients to understand that they are processing the output of V2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c2: Inference API with Versioning\n",
    "\n",
    "We will implement two inference functions. I have already written the business logic of this.\n",
    "\n",
    "`predict_v1` that is accessible via the route \"/v1/predict\". It should accept PredictRequest and output PredictionResponse. \n",
    "\n",
    "`predict_v2` that is accessible via the route \"/v2/predict\". It should accept PredictRequest and output DetailedPredictionResponse. \n",
    "\n",
    "**Optional**: Nerd Stuff: When you have a long running company, you want to have different versions of the model running. For simplicity, we did a simple API versioning using different routes. There are other versioning strategies that does not require new \"routes\". These include: \n",
    "\n",
    "(a) Header-based versioning: Use a custom header (e.g., X-API-Version) to specify the desired version.\n",
    "\n",
    "(b) Query parameter versioning: Use a query parameter (e.g., ?version=2) to specify the version.\n",
    "\n",
    "(c) Content negotiation: Use the Accept header to specify the desired response format and version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c3: Endpoint for Bulk Inference\n",
    "\n",
    "To make things more challenging, let us implement a bulk inference endpoint. This endpoint should accept a file as input and should output a list of DetailedPredictionResponse. Given a file, you should read it and parse it line by line. In the previous inference endpoints, Pydantic already converted the string into a list of float. For this endpoint, we need to do the parsing ourselves. Each line is a string that has space separated input. So convert them into a list of float and pas it to the ML pipeline code (that I have already written). It is possible that the input has some error (eg sending string values as features or not send sufficient number of features etc). In this case, you should raise a HTTPException with the status code 422 and also send the exception details in the message. \n",
    "\n",
    "Details about how to process files can be found [here](https://fastapi.tiangolo.com/tutorial/request-files/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c4: Automatic API Documentation using FastAPI (ungraded)\n",
    "\n",
    "By default, FastAPI provides two endpoints for accessing the documentation:\n",
    "\n",
    "1. Swagger UI: Available at [/docs](http://127.0.0.1:8000/docs)\n",
    "\n",
    "2. ReDoc: Available at [/redoc](http://127.0.0.1:8000/redoc)\n",
    "\n",
    "It should show a neat UI with the API documentation. It even has a nifty utility to test these APIs. It uses some known documentation style. So you can add some comments to the Python function and it will show the documentation automatically. \n",
    "\n",
    "If you do not know what Swagger is, please take some time to learn about Swagger and OpenAPI. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d: Model Serving using Gradio (7.5 points)\n",
    "\n",
    "As the final task, we will experiment [Gradio](https://www.gradio.app/) with that is becoming increasingly popular for demoing ML models. The high level idea is to provide a Pythonic interface so that data scientists can develop web applications without knowing much about frontend development. As you will see, you can create a nice looking UI in 4-5 lines of Python code.\n",
    "\n",
    "I am partial to Gradio as I have used it in many of my demos. However, there are other alternatives too. Other popular ones include [Streamlit](https://streamlit.io/) and [Taipy](https://taipy.io/) that have their tradeoffs. Both are solid alternatives to Gradio. However, Gradio is more popular because of its tight integration with [HuggingFace hub](https://huggingface.co/docs/hub/en/index) where most of the deep learning projects are showcased. \n",
    "\n",
    "The code for this task is inside `task2/gradio_demo.py` . You can test it using the command\n",
    "\n",
    "> rye test -- -m t2gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Specification\n",
    "\n",
    "You can see a simple UI of the code in the screenshot below. The UI should have a DropDown with label \"Classifier\" for the classifier (use the `CLASSIFIER_OPTIONS` variable for creating the values) and a textbox with label \"Features\".\n",
    "\n",
    "There will be two panels on the RHS for the output. If the inputs are correct and there is no error, you should output the class (Negative/Positive) and progress bar to show the respective probabilities. If there is an error, the error will be displayed in the error textbox. \n",
    "\n",
    "The \"business\" logic is written by me. So you can focus on the UI. It should not take more than 5 lines or so for the UI. You also have to change the code a bit to handle the error scenario which should another couple of lines. \n",
    "\n",
    "You can run the application using the command below. This will start a web service rendering that can be accessed (by default) [here](http://127.0.0.1:7860/). Remember to stop the server before running the test suite.\n",
    "\n",
    "> rye run gradio_task\n",
    "\n",
    "\n",
    "\n",
    "You have to use the following Gradio components: Dropdown, Textbox and Label. The Label class can accept class probabilities that my code will already give in the right format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Gradio Success](resources/gradio_success.jpg) \n",
    "\n",
    "![Gradio Failure](resources/gradio_failure.jpg) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
