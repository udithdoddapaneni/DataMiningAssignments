{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Exploration using 2012 FEC Presidential Election Dataset (30 points)\n",
    "\n",
    "In this and next task, you will conduct a simple data exploration over 2012 FEC Presidential Election Dataset. You will learn and use some of the most common exploration/aggregation/descriptive operations. This should also help you learn some of the key functionalities in Pandas. \n",
    "\n",
    "We will also explore some emerging paradigms that will become more popular due to their performance. Specifically, we will explore another cool tool -- duckdb and discuss how it is emerging as an alternative to traditional Pandas based data exploration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "In this assignment, you will use the FEC dataset. The US Federal Election Commission publishes data on contributions to political campaigns. This includes contributor names, occupation and employer, address, and contribution amount. Specifically, we will be using the FEC data from 2012 election between Barack Obama and Mitt Romney. This is widely considered as a landmark election as both sides spent an unprecedented amount of 1 Billion dollars each (i.e. at current exchange rates, more than 8000 Crores in INR each). \n",
    "\n",
    "If you are interested, you can download the entire list of contributor details at the FEC site. It is relatively large (150 MB compressed, 1 GB uncompressed). For our experiments, we will use a smaller subset of the data collected (and cleaned) by Wes McKinney (the creator of Pandas). It is small by most standards (around 1.4 million records, 20 MB compressed, 160 MB uncompressed) but large enough to give a taste of why data mining is compute intensive. Hopefully, this will also give you an appreciation as to the awesomeness of Pandas/Numpy - you can do really cool stuff with 2-3 lines of code that runs in seconds. This also gives us some option to evaluate emerging tools such as Modin and Duckdb that can make analysis even easier. \n",
    "\n",
    "You can download the zipped dataset file [here](https://www.dropbox.com/s/0t5pcnglaf5paoy/fec_2012_contribution_subset.csv.zip?e=2&dl=0). After downloading, please unzip it and put it inside the resources folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "Almost all the tasks below could be purely solved using Pandas. Most analytic tasks should not require more than 2-3 lines of code. Here is a list of functions that you might want to know to solve this assignment (and in general): agg, apply, argmax, argmin, count, crosstab, cumsum, cut, describe, groupby, head, idxmax, idxmin, info, isin, map, max, min, pivot_table, size, sum, transform, unique, value_counts .\n",
    "\n",
    "### Exploration Tasks\n",
    "\n",
    "You can find a set of exploratory analytics tasks below. Ensure that you clearly follow the instructions. The assignment will be graded automatically - so failure to follow might cause some issues. Also do NOT rename the functions or variables that the instructor has set. Of course, feel free to create new variables/functions that will hep your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# These two lines ensure that all modules are reloaded every time a Python cell is executed.\n",
    "# This allows us to modify some other Python file and immediately see the results\n",
    "# instead of restarting the kernel and running every cell. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ds5612_pa1 import t3_tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "The FEC data contains information about all presidential candidates. As a sitting president, Barack Obama was the only candidate from the Democratic party. In the Republican party, there was a process called Primary where number of candidates competed to be the nominee. Mitt Romney won the Republican primary and competed with Barack Obama in the elections, which Obama won.\n",
    "\n",
    "The Python code below reads the FEC dataset into a Pandas data frame with the name fec_all. If your machine has less than 2 GB of RAM, then change the function argument low_memory to True. Once the frame is loaded, we remove all negative contributions (where the campaign refunded amount to a contributor for some reason). Finally, we create a new data frame called fec that contains the contributions to Barack Obama and Mitt Romney alone.\n",
    "\n",
    "For this code to work, the file `fec_2012_contribution_subset.csv` must be in the `resources` folder as the notebook.\n",
    "\n",
    "To reduce my typing, I might refer to Obama as BO and Romney as MR in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please read the code of get_fec_dataset to understand what it is doing\n",
    "fec_df = t3_tasks.get_fec_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Data Exploration using Pandas \n",
    "\n",
    "In this task, we will perform some very high level filtering. Pandas has a convenient and powerful syntax for filtering (for eg, see the code t3_tasks for how I filtered negative contributions and non-Obama, Romney candidates). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1a: Basic Filtering (5 points)\n",
    "\n",
    "We will do some basic filtering. You can test the output using \n",
    "\n",
    "> rye test -- -m t31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama got  135877427.24\n",
      "Romney got  88335907.53\n",
      "CA donated  35062620.839999996\n",
      "NY donated  24836131.14\n",
      "TX donated  12792822.129999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udith/labs and projects/Data Mining Assignments/DS5612_PA1/DS5612_PA1/src/ds5612_pa1/t3_tasks.py:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return fec_df[fec_df[\"cand_nm\"] == candidate][fec_df[\"contbr_employer\"].str.contains(company)][fec_df[\"contbr_occupation\"].str.contains(job)][\"contb_receipt_amt\"].sum()\n",
      "/home/udith/labs and projects/Data Mining Assignments/DS5612_PA1/DS5612_PA1/src/ds5612_pa1/t3_tasks.py:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return fec_df[fec_df[\"cand_nm\"] == candidate][fec_df[\"contbr_employer\"].str.contains(company)][fec_df[\"contbr_occupation\"].str.contains(job)][\"contb_receipt_amt\"].sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google engineers to BO donated  87212.4\n",
      "Google engineers to MR donated  2850.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udith/labs and projects/Data Mining Assignments/DS5612_PA1/DS5612_PA1/src/ds5612_pa1/t3_tasks.py:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return fec_df[fec_df[\"cand_nm\"] == candidate][fec_df[\"contbr_employer\"].str.contains(company)][fec_df[\"contbr_occupation\"].str.contains(job)][\"contb_receipt_amt\"].sum()\n",
      "/home/udith/labs and projects/Data Mining Assignments/DS5612_PA1/DS5612_PA1/src/ds5612_pa1/t3_tasks.py:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return fec_df[fec_df[\"cand_nm\"] == candidate][fec_df[\"contbr_employer\"].str.contains(company)][fec_df[\"contbr_occupation\"].str.contains(job)][\"contb_receipt_amt\"].sum()\n"
     ]
    }
   ],
   "source": [
    "#Task 3.1a1: Let us find out how much contributions did Obama and Romney made in this dataset\n",
    "# Remember, that this is not the complete amount as it excludes other sources like PACs, Super PACs and \n",
    "#  spending by party committes.\n",
    "#Hint: use cand_nm field\n",
    "\n",
    "# Modify the function t3_tasks.tot_amount_candidate_pandas to return the amount\n",
    "\n",
    "print(\"Obama got \", t3_tasks.tot_amount_candidate_pandas(fec_df, \"Obama, Barack\"))\n",
    "print(\"Romney got \", t3_tasks.tot_amount_candidate_pandas(fec_df, \"Romney, Mitt\"))\n",
    "\n",
    "\n",
    "#Task 3.1a2: How much contribution did folks from \n",
    "# California, New York and Texas make totally (i.e. to both Obama and Romney).\n",
    "#Hint: use contbr_st field\n",
    "\n",
    "# Modify the function t3_tasks.tot_amount_state_pandas to return the amount\n",
    "\n",
    "print(\"CA donated \", t3_tasks.tot_amount_state_pandas(fec_df, \"CA\"))\n",
    "print(\"NY donated \", t3_tasks.tot_amount_state_pandas(fec_df, \"NY\"))\n",
    "print(\"TX donated \", t3_tasks.tot_amount_state_pandas(fec_df, \"TX\"))\n",
    "\n",
    "\n",
    "\n",
    "#Task 3.1a3: How much did Engineers from Google gave to BO and MR.\n",
    "# This task is a bit tricky as there are many variations: \n",
    "# eg, SOFTWARE ENGINEER vs ENGINEER and \n",
    "# GOOGLE INC. vs GOOGLE\n",
    "# So you need to use substring matching for both job name and company name\n",
    "\n",
    "# Modify the function t3_tasks.tot_amount_job_pandas to return the amount\n",
    "\n",
    "print(\"Google engineers to BO donated \",\n",
    "      t3_tasks.tot_amount_job_pandas(fec_df, \"Obama, Barack\", \"GOOGLE\", \"ENGINEER\"))\n",
    "print(\"Google engineers to MR donated \",\n",
    "      t3_tasks.tot_amount_job_pandas(fec_df, \"Romney, Mitt\", \"GOOGLE\", \"ENGINEER\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1b: Aggregation+Filtering+Ranking  (5 points)\n",
    "\n",
    "In this task, we will perform some very high level aggregation followed by filtering and ranking. Pandas has some convenient functions for aggregation (do NOT write a for loop - Pandas has some very efficient, vectorized code). Pandas is often quite clever and might sort the data for you already.  \n",
    "\n",
    "Hint: Pandas has ready made functions for all the following. So your function will be a one-liner.\n",
    "\n",
    "You can test the output using \n",
    "\n",
    "> rye test -- -m t31b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cmte_id', 'cand_id', 'cand_nm', 'contbr_nm', 'contbr_city',\n",
      "       'contbr_st', 'contbr_zip', 'contbr_employer', 'contbr_occupation',\n",
      "       'contb_receipt_amt', 'contb_receipt_dt', 'receipt_desc', 'memo_cd',\n",
      "       'memo_text', 'form_tp', 'file_num'],\n",
      "      dtype='object')\n",
      "contbr_st\n",
      "AA       74\n",
      "AB        4\n",
      "AE      395\n",
      "AK     2036\n",
      "AL     3854\n",
      "      ...  \n",
      "WA    20783\n",
      "WI     8050\n",
      "WV     1330\n",
      "WY     1055\n",
      "ZZ       15\n",
      "Name: contbr_nm, Length: 64, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Task 3.1b: Print the number of contributors to a given candidate in each state.\n",
    "# For simplicity, assume that each distinct name (contbr_nm) is a unique contributor.\n",
    "# Hint: this should return a series object\n",
    "\n",
    "# Modify tot_contributions_for_cand_pandas to output this.\n",
    "\n",
    "print(t3_tasks.tot_contributions_for_cand_pandas(fec_df, \"Obama, Barack\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top-10 states with most contributors to Obama are :\n",
      "\n",
      "contbr_st\n",
      "CA    100182\n",
      "NY     50383\n",
      "IL     33240\n",
      "TX     32292\n",
      "FL     29797\n",
      "MA     24864\n",
      "MD     22552\n",
      "VA     21451\n",
      "WA     20783\n",
      "PA     19280\n",
      "Name: contb_receipt_amt, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Task 3.2b: Print the top-10 states (based on number of contributors) \n",
    "# that contributed to a given candidate.\n",
    "# print both state name and number of contributors\n",
    "t3_2b_top10_obama_contr_states = t3_tasks.top_10_state_pandas(fec_df, 'Obama, Barack')\n",
    "print(\"\\n\\nTop-10 states with most contributors to Obama are :\\n\")\n",
    "print(t3_2b_top10_obama_contr_states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1c Discretization  (5 points)\n",
    "\n",
    "Similar to India, big money dominates US politics. Let us do a very analysis via discretization where we create buckets and put contributions based on the buckets. Discretization in Pandas is acheived by cut function.\n",
    "\n",
    "You can test the output using \n",
    "\n",
    "> rye test -- -m t31c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udith/labs and projects/Data Mining Assignments/DS5612_PA1/DS5612_PA1/src/ds5612_pa1/t3_tasks.py:58: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  return ans.unstack(0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cand_nm</th>\n",
       "      <th>Obama, Barack</th>\n",
       "      <th>Romney, Mitt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 1]</th>\n",
       "      <td>318.24</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 10]</th>\n",
       "      <td>337267.62</td>\n",
       "      <td>29819.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 100]</th>\n",
       "      <td>20288981.41</td>\n",
       "      <td>1987783.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(100, 1000]</th>\n",
       "      <td>54798531.46</td>\n",
       "      <td>22363381.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000, 10000]</th>\n",
       "      <td>51753705.67</td>\n",
       "      <td>63942145.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10000, 100000]</th>\n",
       "      <td>59100.00</td>\n",
       "      <td>12700.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(100000, 1000000]</th>\n",
       "      <td>1490683.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1000000, 10000000]</th>\n",
       "      <td>7148839.76</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cand_nm              Obama, Barack  Romney, Mitt\n",
       "bins                                            \n",
       "(0, 1]                      318.24         77.00\n",
       "(1, 10]                  337267.62      29819.66\n",
       "(10, 100]              20288981.41    1987783.76\n",
       "(100, 1000]            54798531.46   22363381.69\n",
       "(1000, 10000]          51753705.67   63942145.42\n",
       "(10000, 100000]           59100.00      12700.00\n",
       "(100000, 1000000]       1490683.08          0.00\n",
       "(1000000, 10000000]     7148839.76          0.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Task 3.1c:\n",
    "# In this task, let us compute the TOTAL AMOUNT in each bucket.\n",
    "# Specifically, compute for each candidate the total amount of contributions \n",
    "# that each got in each bucket.\n",
    "# Suppose Obama got contributions such as 2, 6, 16, 18, 120\n",
    "#  0 in (0,1], 2+6=8 in (1,10] , 16+18=34 in (10, 100] and 120 in (100, 1000]\n",
    "# Hint: This could be done in 2-3 lines\n",
    "\n",
    "# Here is the step by step algorithm\n",
    "# First, use cut to get the discretization bin \"labels\" \n",
    "# Second, use this information to do a groupby\n",
    "# Third, in this groupby object, do a sum over the contribution amount.\n",
    "# Finally, use the unstack(0) function over the groupby sum object\n",
    "\n",
    "t3_tasks.discretization_pandas(fec_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Data Exploration using DuckDB \n",
    "\n",
    "The goal of this task is to introduce to the cool tool of DuckDB. Please read the [Why DuckDB](https://duckdb.org/why_duckdb) for more details. \n",
    "\n",
    "DuckDB is a fantastically fast database with ton of cool features. Probably the coolest is that it is a single file binary and kinda-sorta serverless. In other words, you do not need a separate process to be running to handle queries such as for Postgres, MySQL etc. This might sound like a weird choice but it has lot of advantages. The technical term for this is embedded operations.\n",
    "\n",
    "DuckDB piggybacks on top of an existing host process and silently performs the querying. The efficient vectorized implementation sometimes allows it to be blazingly fast. For example, you can use DuckDB to query a pandas data frame and can do complex operations that is much faster than pandas can do it. Since DuckDB resides in the same process, it avoids copying data resulting in blazing fast operations. \n",
    "\n",
    "Tools such as DuckDB and ClickHouse are becoming more and more popular for performing data analytics. While an extensive intro to DuckDB is beyond the scope of this assignment, we will do couple of simple tasks to evaluate its potential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2a: Creating and Querying In-memory DuckDB Database (5 points)\n",
    "\n",
    "In this single sub task, you will be implementing  multiple interesting ideas of DuckDB in one shot. \n",
    "\n",
    "First, we will be creating a DuckDB database in-memory. Traditional database such as Postgres have a complex way of storing the data. Embedded databases such as SQLite and DuckDB are much simpler. All the data resides in a single file. DuckDB does something even cool. It has a set of adaptors that allows you to use the DuckDB engine for querying and get the data from somewhere else. For example, you can query a csv file or a SQLite file or a Postgres database without creating an intermediate DuckDB database. In this task, we will create an in-memory database of the FEC dataset.\n",
    "\n",
    "Second, we will do something called schema less loading. The traditional way of creating a database from a CSV file will be to create a DB with a given schema, read the CSV file line and then insert that line into the DB. This adds friction. DuckDB allows you to create a DB from csv file directly. It uses lot of heuristics to infer the schema and data types. Hint: check the `CREATE TABLE AS SELECT *` and the `read_csv_auto` functions.\n",
    "\n",
    "Third, we will do some basic querying over this constructed database. Specifically, we will do couple of the tasks that you did for the pandas version. \n",
    "\n",
    "You will be implementing this inside `load_fec_data_to_duckdb` of `t3_tasks.py`. This function accepts the path to the CSV file. You should create an in-memory database, populate it with data from the input file and then return the database connection. Note that the name of the table **HAS** to be `fec_table`. No need to do any filtering (such as removing negative contributions that we did not pandas). It is essential to use the csv file name variable as the grader might test it with other files.\n",
    "\n",
    "You can test the output using \n",
    "\n",
    "> rye test -- -m t32a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2b: Querying in-memory DuckDB database (5 points)\n",
    "\n",
    "Next, we will use the loaded database using the duckdb's Python client. We will do a simple evaluation where we will translate couple of tasks from the Pandas questions to duckdb. \n",
    "\n",
    "You will write the function query_fec_data that accepts two parameters detailed below. The code should run the query on con and return the entire output. Hint: It is a one-liner.\n",
    "- `con` which is a duckdb connection where the data is already loaded\n",
    "- `query` which gives a fully fleshed out SQL query. \n",
    "\n",
    "\n",
    "We will run couple of simple SQL queries. \n",
    "1. Update the `t32b1_query` variable with the SQL query that computes the total contribution amount ONLY for Obama and Romney. Order the results based on the candidate name. Also remember to do this aggregation for positive contributions. (See the function `get_fec_dataset` for the Pandas version of this pre-processing)\n",
    "\n",
    "2. Update the `t32b2_query` variable with the SQL query needed for Task 3.2b. Output the top-10 states with the most number of contributors who donated to Obama. Once again, do not forget to filter for the positive contributions and sort the groups in descending order based on the number of contributors. \n",
    "\n",
    "You can test the output using \n",
    "\n",
    "> rye test -- -m t32b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.2c: Querying Pandas Data using DuckDB (5 points)\n",
    "\n",
    "This task is relatively easy but you have to spend few minutes understanding the conventions. As mentioned above, DuckDB is an embedded system that is loaded inside the Python process and has access to all the major variables. We will do a simple task to demonstrate how to run a SQL query against a pandas DB.\n",
    "\n",
    "We will use the same task as the one you did for t32b2_query. Set the `t32c1_query` variable with the updated query from `t32b2_query`. You can assume that a Python variable called `fec_df` is available with the right data. Hint: the queries t32b2_query and t32c1_query will be almost identical. \n",
    "\n",
    "You can test the output using \n",
    "\n",
    "> rye test -- -m t32c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
